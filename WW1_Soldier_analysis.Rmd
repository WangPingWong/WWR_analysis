---
title: "Analysis of aDNA WW1 soldier"
author: "Wang Ping"
date: "11/2/2020"
output: 
    html_document:
      css: "style/style.css"
      toc: true
      toc_float: true
      toc_depth: 3
      highlight: pygments
      number_sections: false
      code_folding: hide
#      keep_md: true
bibliography: style/aDNA.bib
csl: style/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
# load custom functions from github
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "util.R")
# options(width = 180)
cran_packages <- c("dplyr", "knitr", "pander","kableExtra", "captioner", "DT", "htmltab",
                   "paletteer", "dartR", "poppr")
pacman::p_load(char=cran_packages, repos="https://cran.rstudio.com/")
custom_font="consolas"
fontFmt = function(x,font="consolas"){
  #outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  #if (outputFormat == 'html')
  formatted_text <- sprintf("<font face='%s'>%s</font>",font,x)
  return(formatted_text)
  #else
  #  x
}
```

# Analysis Pipeline
## General overview:

1. Data pre-processing:  
    a. Quality check  
    b. Adaptor trimming  
    c. Post-trim quality check  
2. Mapping reads to a reference genome (keep unmapped)  
3. Reads deduplication  
4. Variant calling and filtration  
5. Produce variant statistics and assessment  
6. Match SNPs with existing databases (1000 genome European set)  

### Ancient DNA specific considerations
1. Assess insert size with BBmerge/BBmap (see [this thread](https://www.biostars.org/p/284302/))  
2. Align to the Human genome (GRCh38) with bwa/Bowtie2  
3. Extract alignment to Mitochondrial genome and Y chromosomes to check levels of alignment and assign haplogroups   
4. Assess alignment with Qualimap (Maybe treat as single-end if too many pairs are being dropped).  
5. Assess DNA damage with MapDamage   
6. Call SNPs from nuclear genome  
7. Create pseudo-haploid representation of the genome?


## Methods
DNA-Seq data processing, mapping and variant calling were performed on the _Griffith University HPC Cluster_ (using Torque PBS scheduler), following the methods specified by (see details in Appendix 1), .  
Detailed methods, including code for running each of the analyses steps are provided in the associated [WWR_analysis GitHub repository](https://github.com/WangPingWong/WWR_analysis).


### Server access
Command to login to the server (make sure that the VPN is connected)
```{bash server_access}
ssh -Y s5119482@10.250.250.3
# or
ssh -Y s5119482@gc-prd-hpclogin1.rcs.griffith.edu.au
```  

### Setup Bioinformatics Environment
Install conda environment to setup bioinformatics tools needed
```{bash conda_setup}
# copy startup files (allow internet access)
cp /scratch/aDNA/startup_files/.* ~/
# download conda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
# install conda
bash Miniconda3-latest-Linux-x86_64.sh 
# initialise conda
source ~/.bashrc
# install extra packages to the base environment
conda install libgcc gnutls libuuid readline cmake git tmux libgfortran parallel gawk pigz rename 
# update conda
conda update --all -y -n base

# install bioinformatics software
CONDA_NAME=aDNA #align for alignments and popgen for popgen analysis
conda create -y -n $CONDA_NAME
conda install -y -n $CONDA_NAME blast fastqc multiqc bioawk bbmap bowtie2 bwa bwa-mem2 cutadapt trimmomatic samtools samblaster biobambam sambamba=0.6.6 picard sequencetools mapdamage2 nextflow qualimap mawk p7zip pandas numpy
# create an environment to process and analyse variants
conda create -n snp_tools samtools vcftools bcftools gatk4=4.1.9.0 freebayes snpsift stack plink admixtools admixture structure eigensoft vcfanno vt # scikit-learn=0.18.2
# conda create -n finestructure finestructure fineradstructure 
# conda create -n faststructure faststructure
# activate environment and cleanup
conda activate $CONDA_NAME
# Clean extra space
conda clean -y --all


```

### Useful Linux Commands
1. List folder content - `ls`, or `ls -lha` to display also hidden files in a detailed format  
2. Change directory - `cd`, e.g. `cd ~` (to home folder)
3. Option+C - force stop all the commands  
4. Create new directories - `mkdir`, e.g. `mkdir analysis_WW1`, the folder name will be analysis_WW1  
5. Check (https://www.educative.io/blog/bash-shell-command-cheat-sheet) for 25 most common commands
6. `tmux` - start a new session; `Ctrl+B c` to create a new tab (window); `Ctrl+B n` to move to the next tab; `Ctrl+B p` to move to the previous tab; `Ctrl+B [` for scroll mode (use keyboard arrows, `q` to exit mode); `Ctrl+B d` to detach a session; and `tmux a -t0` to return to an active session (who's number is 0).  (see [cheatsheet](https://gist.github.com/MohamedAlaa/2961058))

### Running the Analysis Pipeline
#### Downloading reference genome files
Download raw `.fastq.gz` files from NECTAR server and human genome reference (GRCh38) files from NCBI.
_Make sure that the assembly (`.fna`) and annotation (`.gff`) files match the same assembly_

```{bash retrieve_files}
REF_DIR=$HOME/aDNA/Human/reference_genomes
mkdir -p $REF_DIR  && cd $REF_DIR # on new GU HPC (shared aDNA folder)

# download from ENSEMBL (GRCh38.p13)
wget ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz
wget ftp://ftp.ensembl.org/pub/release-101/gff3/homo_sapiens/Homo_sapiens.GRCh38.101.gff3.gz

```

#### Mapping to the reference genome 
The quality-trimmed reads were mapped to the Human reference genome, [GRCh38.p13](https://asia.ensembl.org/Homo_sapiens/Info/Index) using `r fontFmt("Bowtie 2")` v2.3.5.1, while adding Read Group information [@langmeadFastGappedreadAlignment2012]. `r fontFmt("Bowtie 2")` was run with custom parameteres for alignment of ancient DNA reads: local mode (`--local`), allowing a single mismatch in the mapping seed (`-N 1`) and a maximum mismatch penalty of 4 (`--mp 4`) [@cahillGenomicEvidenceWidespread2018]. The resulting alignments were processed to mark duplicate reads with `bamsormadup` from `r fontFmt("biobambam2")` [@tischlerBiobambamToolsRead2014].  
To improve performance, temporary files were written to a local `scratch` folder on the computing node. Mapping statistics (rate, quality and coverage) were evaluated with `r fontFmt("Qualimap 2")` v.2.2.2-dev [@okonechnikov_qualimap_2016].

Y-chromosome haplogroup was predicted from the alignment files using [Yleaf](https://github.com/genid/Yleaf) v2.2, using default parameters [@ralfYleafSoftwareHuman2018].


```{bash aDNA_bt2}
CONDA_NAME=aDNA
conda activate $CONDA_NAME
# Prepare the variables that will be used.
DATE=`date +%d_%m_%Y`
BATCH=WWR
ALIGNER=BT2
RUN="${BATCH}_${ALIGNER}_process_${DATE}" # day of run was 26_11_2020
# Create tmp folder
TMP_DIR=/scratch/$USER/tmp/$RUN  && mkdir -p $TMP_DIR # on GU HPC
# Create working folder for the run
mkdir -p ~/aDNA/Human/WWR/${RUN} # on GU HPC
cd !$
REF_DIR="$HOME/aDNA/Human/reference_genomes" # on GU HPC
REF="GRCh38"
GENOME="$REF_DIR/$REF"
[ ! -s "$REF_DIR/$REF.fasta" ] && pigz -cd $(find $REF_DIR -name "*$REF*.f*a.gz") > $GENOME.fasta
[ ! -s "$REF_DIR/$REF.gff" ] && pigz -cd $(find $REF_DIR -name "*$REF*.gff.gz") > $GENOME.gff
# create index and dictionary for the reference genome
samtools faidx $GENOME.fasta
picard CreateSequenceDictionary R=$GENOME.fasta
#cat $REF_DIR/Dingo_MT_MH035676.1.fasta >> $REF_DIR/$REF.fasta


READ_LENGTH=150
NCORES=16 # use maximum 16!
NNODES=1
RGPM=HiSeqX 
RGPL=ILLUMINA
RGPU=unit1
RGCN=AGRF
FQ_DIR="$HOME/aDNA/Human/WWR"
# mkdir trimmed_reads
ln -s $FQ_DIR/*.f*q.gz ./

# Prepare PBS script
echo '#!/bin/bash 
#PBS -V
#PBS -l' "select=${NNODES}:ncpus=${NCORES}:mem=16GB,walltime=20:00:00
#PBS -W umask=002

# mkdir -p $TMP_DIR
cd \$PBS_O_WORKDIR
source ~/.bashrc
conda activate $CONDA_NAME
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > ${RUN}.pbspro

# Build bowtie2 index
ALIGN_IDX_ID=$( echo "cd $( pwd ) ; source ~/.bashrc; conda activate $CONDA_NAME; bowtie2-build ${GENOME}.fasta ${GENOME}" | qsub -V -l select=1:ncpus=${NCORES}:mem=16GB,walltime=3:00:00 -N index-ref | egrep -o "^[0-9]+") # 70759.pbsserver
# BWA_IDX_ID=$( echo $BWA_IDX_ID | grep -P -o '[\d\.]+(?!\.\w)' | sed 's/\.$//' )

# bwa mem -M  -t 2 ${REF_GENOME} ${INPUT_DIR}/${sample}.fq.gz | samtools view -bS - > ${OUTPUTDIR}/${sample}.bam
# Perform adapter trimming, mapping, deduplication, add RG, sort and index 
find ./ -maxdepth 1 -name "UWW1*_1.f*q.gz" | sort | gawk -F"\t" -v ref=$REF -v ref_dir=$REF_DIR -v RGPL=$RGPL -v RGPU=$RGPU -v RGCN=$RGCN -v RGPM=$RGPM -v ncores=$NCORES -v mapper=${ALIGNER} -v tmpdir=$TMP_DIR '{n=split($1,a,"/"); infile2=gensub("_1\\.", "_2.", "1", $1); sample_id=gensub("_1.f.*q.gz", "", "1", a[n]); printf("bowtie2 --local -p %s --mp 4 -N 1 -x %s/%s -1 %s -2 %s --un-conc-gz %s_unmapped_pairs.fq.gz --rg-id %s --rg SM:%s --rg LB:%s --rg PL:%s --rg PM:%s --rg PU:%s --rg CN:%s | bamsormadup inputformat=sam level=8 templevel=3 tmpfile=%s/%s.tmp M=%s.dedup.metrics indexfilename=%s.%s.%s.dedup.rg.csorted.bai > %s.%s.%s.dedup.rg.csorted.bam && rm -r %s/%s.tmp*\n", ncores, ref_dir,ref, $1, infile2, sample_id, sample_id, sample_id, sample_id, RGPL,RGPM, RGPU, RGCN, tmpdir, sample_id, sample_id, sample_id,mapper,ref, sample_id,mapper,ref, tmpdir, sample_id)}' > ${RUN}.bash

# Run on parallel nodes (faster and preferred)
# fix picard -XX:ParallelGCThreads=n
# Run the commands 
JOB_NAME=${RUN}
JOBS_NUM=$( cat ${RUN}.bash | wc -l )
ALIGN_ID=$( qsub -J1-$JOBS_NUM -N ${JOB_NAME:0:11} -W depend=afterok:$ALIGN_IDX_ID -vCMDS_FILE=${RUN}.bash  ${RUN}.pbspro | egrep -o "^[0-9]+") 
#BWA_ID=$( echo $BWA_ID | grep -P -o '[\d\.]+(?!\.\w)' | sed 's/\.$//'  )
# Record the array ID: 70761[] on GU HPC

# Run qualimap on bam files (files need to be sorted)
QUALIMAP_JOB="qualimap_${RUN}"
# Create/copy a file describing the sample names (sample_info.txt) to the parent folder and use it in qualimap 
QUALIMAP_JOB_ID=$( echo "cd $( pwd ) ; source ~/.bashrc; conda activate $CONDA_NAME; unset DISPLAY ; ls -1 UWW1*${ALIGNER}.${REF}.dedup.rg.csorted.bam | gawk '{sample_name=gensub(/\\.${ALIGNER}.${REF}.dedup.rg.csorted.bam/, \"\", \$1); printf \"%s\\t%s\n\", sample_name, \$1}'   > $QUALIMAP_JOB.samples ; qualimap multi-bamqc --java-mem-size=30G -r -c -gff $GENOME.gff -d $QUALIMAP_JOB.samples -outformat PDF:HTML -outfile $QUALIMAP_JOB.qualimap.multisampleBamQcReport -outdir $QUALIMAP_JOB ; cp $QUALIMAP_JOB/multisampleBamQcReport.html $QUALIMAP_JOB/$QUALIMAP_JOB.qualimap.multisampleBamQcReport.html" | qsub -V -l select=1:ncpus=${NCORES}:mem=32GB,walltime=7:00:00 -N ${QUALIMAP_JOB:0:11} -W depend=afterok:$ALIGN_ID[] | egrep -o "^[0-9]+") # 70762.pbsserv

# collect mapping stats - update to work with Bowtie2
echo '#!/bin/bash 
#PBS -V
#PBS -l' "select=1:ncpus=8:mem=8GB,walltime=00:30:00

cd \$PBS_O_WORKDIR
source ~/.bashrc
conda activate $CONDA_NAME

cat ${JOB_NAME:0:11}.e* > ${RUN}.log
# collect mapping stats
egrep 'Created read group|overall alignment rate|samblaster: Marked'  ${RUN}.log | gawk -v ORS="\t" -F " " 'BEGIN{printf \"sample_id\tmapping_rate\ttotal_reads\tduplicated_reads\n\"} NR%3==1{match(\$0, /.+ID=(.+?) PL.+/, a); print a[1]} NR%3==2{print \$1} NR%3==0{printf \"%s\t%s\n\", \$5, \$3}'   > ${RUN}_$REF.stats" > ${RUN}_stats.pbspro 
# | paste ${RUN}.stats.tmp -
STATS_JOB_ID=$(qsub ${RUN}_stats.pbspro -W depend=afterok:$ALIGN_ID[] | egrep -o "^[0-9]+")
# merge bams
IGNORE_SAMS='NONE'

# get Y chromosome haplogroup
YLEAF_JOB="Yleaf_${RUN}"

echo '#!/bin/bash 
#PBS -V
#PBS -l' "select=1:ncpus=8:mem=16GB,walltime=02:00:00
#PBS -W umask=002

cd \$PBS_O_WORKDIR
source ~/.bashrc
conda activate $CONDA_NAME
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > ${YLEAF_JOB}.pbspro

ls -1 UWW1*${ALIGNER}.${REF}.dedup.rg.csorted.bam | gawk -v ncores=8 '{sample_name=gensub(/\..+.dedup.rg.csorted.bam/, "", $1); printf "~/aDNA/Tools/Yleaf/Yleaf.py -t %s -bam %s -pos ~/aDNA/Tools/Yleaf/Position_files/WGS_hg38.txt -out %s.yleafout -r 1 -q 20 -b 90 && ~/aDNA/Tools/Yleaf/predict_haplogroup.py -input %s.yleafout/ -out %s.yleafout/%s_yleaf.hg\n", ncores, $1, sample_name, sample_name, sample_name, sample_name}' > ${YLEAF_JOB}.bash

YLEAF_ID=$( qsub -J1-$(cat ${YLEAF_JOB}.bash | wc -l) -N ${YLEAF_JOB:0:11} -W depend=afterok:$ALIGN_ID[] -vCMDS_FILE=${YLEAF_JOB}.bash  ${YLEAF_JOB}.pbspro | egrep -o "^[0-9]+") 

IGNORE_SAMS="NONE"
# merge BAMS to a single file (not needed for GATK)
# BAMS=$( ls -1 ./*.csorted.bam | egrep -v $IGNORE_SAMS | gawk -v ORS=" " '{print $1}' )
# FINAL_MERGE_ID=$( echo "cd $( pwd ) ; source ~/.bashrc; conda activate $CONDA_NAME; sambamba merge -t $NCORES -l 8 ${BATCH}.${ALIGNER}.${REF}.all.csorted.combined.bam $BAMS && sambamba index ${BATCH}.${ALIGNER}.${REF}.all.csorted.combined.bam && sambamba view -H ${BATCH}.${ALIGNER}.${REF}.all.csorted.combined.bam | grep "^@RG" > ${BATCH}.${ALIGNER}.${REF}.all.read_groups.txt" | qsub -V -l select=1:ncpus=${NCORES}:mem=16GB,walltime=5:00:00 -N Final_merge -W depend=afterok:$ALIGN_ID[]:$QUALIMAP_JOB_ID | egrep -o "^[0-9]+") 


# CALL_VARS="pileup_${RUN}"
# CALL_VARS_ID=$( echo "cd $( pwd ) ; source ~/.bashrc; conda activate $CONDA_NAME; bcftools mpileup -d 1000 -Ou -f $GENOME.fasta ${BATCH}.${ALIGNER}.${REF}.all.csorted.combined.bam |   bcftools call -Ou -mv  | bcftools norm --threads \$NCPUS -f $GENOME.fasta -Oz > ${BATCH}.${ALIGNER}.${REF}.norm.vcf.gz && bcftools index ${BATCH}.${ALIGNER}.${REF}.norm.vcf.gz " | qsub -V -l select=1:ncpus=${NCORES}:mem=8GB,walltime=50:00:00 -N ${CALL_VARS:0:11} -W depend=afterok:$FINAL_MERGE | egrep -o "^[0-9]+")

# GATK Hc
GATK_HC="gatk_hc_$(date +%d_%m)"
# -L MT
ls -1 UWW1*${ALIGNER}.${REF}.dedup.rg.csorted.bam | gawk -v genome=$GENOME -v ref=$REF -v ncores=$NCORES -v mapper=${ALIGNER} -v tmpdir=$TMP_DIR  '{sample_name=gensub(/\..+.dedup.rg.csorted.bam/, "", $1); printf "gatk --java-options "-Xmx12g" HaplotypeCaller -R %s.fasta -I %s -O %s.HC.g.vcf.gz -ERC GVCF\n", genome, $1, sample_name}' > $GATK_HC.bash

# create a pbs file
#sed -E 's/mem=[0-9]+GB,walltime=[0-9]+:00:00/mem=8GB,walltime=15:00:00/'   ${RUN}.pbspro > $GATK_HC.pbspro

GATK_HC_ID=$( qsub -J1-$(cat ${GATK_HC}.bash | wc -l) -N ${GATK_HC:0:11} -l select=1:ncpus=1:mem=16GB,walltime=15:00:00 -vCMDS_FILE=${GATK_HC}.bash  ${RUN}.pbspro | egrep -o "^[0-9]+") 

# combine gvcf files
GVCF_FILES=$(ls -1 UWW1*${ALIGNER}.${REF}.dedup.rg.csorted.bam | gawk '{sample_name=gensub(/\..+.dedup.rg.csorted.bam/, "", $1); printf "-V %s.HC.gvcf ",sample_name}' )
# CHRS=$(gawk -vORS="," '{print $1}' $GENOME.fasta.fai )
cut -f1 $GENOME.fasta.fai > ${REF}_intervals.list 

# Create a GenomicsDB workspace (need to do this per chromosome)
# for each chromosome
# check what is the size of all the partial contigs
gawk 'BEGIN{tot=0};$1~/^GL/{tot+=$2};END{print tot}'  $GENOME.fasta.fai 
gawk 'BEGIN{tot=0};$1~/^KI/{tot+=$2};END{print tot}'  $GENOME.fasta.fai 
# send them to a file
gawk '$1~/^GL|^KI/{print $1}' $GENOME.fasta.fai > ${REF}_contigs.list

# create a general array pbs file
echo '#!/bin/bash 
#PBS -V
#PBS -W umask=002

cd $PBS_O_WORKDIR
source ~/.bashrc'"
conda activate $CONDA_NAME
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > array.pbspro

# create a command for each chromosome
egrep -v "^GL|^KI" $GENOME.fasta.fai | cut -f1 | parallel --dry-run "export TILEDB_DISABLE_FILE_LOCKING=1; gatk GenomicsDBImport --java-options \"-Xmx4g -Xms4g\" $GVCF_FILES --genomicsdb-workspace-path GenomicsDB_{} --tmp-dir $TMP_DIR -L {} " > DBimport_${BATCH}.bash
# add the contigs to the commands file
echo "export TILEDB_DISABLE_FILE_LOCKING=1; gatk GenomicsDBImport --java-options \"-Xmx4g -Xms4g\" $GVCF_FILES --genomicsdb-workspace-path GenomicsDB_contigs --tmp-dir $TMP_DIR -L ${REF}_contigs.list " >> DBimport_${BATCH}.bash

# create workspace for each chromosome (in parallel via the cluster)
DBIMPORT_ID=$( qsub -J1-$(cat DBimport_${BATCH}.bash | wc -l) -l select=1:ncpus=1:mem=6GB,walltime=2:00:00 -N gatk_dbimport -vCMDS_FILE=DBimport_${BATCH}.bash  array.pbspro | egrep -o "^[0-9]+")

# can verify that all jobs finished successfully with
grep "Import completed" *.e$DBIMPORT_ID*

# Call variants from each chromosome/workspace
egrep -v "^GL|^KI" $GENOME.fasta.fai | cut -f1 | parallel --dry-run "gatk GenotypeGVCFs --java-options \"-Xmx12g -Xms12g\" -R $GENOME.fasta -V gendb://GenomicsDB_{} --tmp-dir $TMP_DIR -O ${BATCH}.${ALIGNER}.${REF}.{}.vcf.gz" > GenotypeGVCFs_${BATCH}.bash

echo "gatk GenotypeGVCFs --java-options \"-Xmx12g -Xms12g\" -R $GENOME.fasta -V gendb://GenomicsDB_contigs --tmp-dir $TMP_DIR -O ${BATCH}.${ALIGNER}.${REF}.contigs.vcf.gz" >> GenotypeGVCFs_${BATCH}.bash

GENOVCF_ID=$( qsub -J1-$(cat GenotypeGVCFs_${BATCH}.bash | wc -l) -l select=1:ncpus=2:mem=16GB,walltime=20:00:00 -N GenotypeGVCFs  -vCMDS_FILE=GenotypeGVCFs_${BATCH}.bash ${RUN}.pbspro | egrep -o "^[0-9]+")

# can verify that all jobs finished successfully with
grep "Traversal complete" *.e$GENOVCF_ID*

# combine all into one file!! (need to have vcftools installed in environment)
vcf-concat ${BATCH}.${ALIGNER}.${REF}*vcf.gz | bgzip > ${BATCH}.${ALIGNER}.${REF}.vcf.gz
# index that file
bcftools index ${BATCH}.${ALIGNER}.${REF}.vcf.gz

# call variants from just MT (not needed if we have the entire VCF)
# GENOVCFMT_ID=$(echo "cd $( pwd ) ; source ~/.bashrc; conda activate $CONDA_NAME; gatk GenotypeGVCFs --java-options \"-Xmx4g\" -R $GENOME.fasta -V gendb://${BATCH}_GenomicsDB -L MT -new-qual -O ${BATCH}.all.${ALIGNER}.${REF}.MT.vcf"  | qsub -V -l select=1:ncpus=4:mem=4GB,walltime=100:00:00 -N GenotypeMT -W depend=afterok:$DBIMPORT_ID | egrep -o "^[0-9]+")

# Extract mitocondrial sequences
samtools faidx $GENOME.fasta
# commands to extract the consensus sequence for each mitochondrial (MT) contig
parallel "samtools faidx $GENOME.fasta MT | bcftools consensus -H 1pIu -s {} ${BATCH}.${ALIGNER}.${REF}.vcf.gz > {}.chrMT.fa" ::: \$(zcat ${BATCH}.${ALIGNER}.${REF}.vcf.gz | head -n 2000 | grep '#CHROM' | cut -f 10-) 
    
# VariantFiltration
conda activate $CONDA_NAME
# filter with SnpSift to make sure that all genotypes have DP>5
zcat ${BATCH}.${ALIGNER}.${REF}.vcf.gz | SnpSift filter "( GEN[?].DP > 4 )" | pigz -c > ${BATCH}.${ALIGNER}.${REF}.DP5.vcf.gz

# Add genotype quality to the filter
zcat ${BATCH}.${ALIGNER}.${REF}.vcf.gz | SnpSift filter "( ! GEN[*].DP < 5 ) & ( ! GEN[*].GQ < 30 )" | pigz -c > ${BATCH}.${ALIGNER}.${REF}.DP5.vcf.gz

#gatk VariantFiltration -R $GENOME.fasta -V myfile_HC.vcf --filter-expression "DP<5" --filter-name "DP" -O myfile_HC_VF.vcf

#FINAL_MERGE=$( echo "cd $( pwd ) ; picard MergeSamFiles USE_THREADING=true SO=coordinate $BAMS O=all.csorted.combined.bam" | qsub -V -l select=1:ncpus=${NCORES}:mem=96GB,walltime=5:00:00 -N Final_merge ) # 5248672.pbsserver
# FINAL_MERGE_ID=$( echo $FINAL_MERGE | grep -P -o '[\d\.]+(?!\.\w)' | sed 's/\.$//')

# save read groups to file (useful for vcf later on)



MT_CONSENS_ID=$( echo "cd $( pwd ) ; source ~/.bashrc; conda activate $CONDA_NAME; parallel 'samtools faidx $GENOME.fasta MT | bcftools consensus -H 1pIu -s {} ${BATCH}.${ALIGNER}.${REF}.vcf.gz > {}.chrMT.fa' ::: \$(zcat ${BATCH}.${ALIGNER}.${REF}.vcf.gz | head -n 2000 | grep '#CHROM' | cut -f 10- ) " | qsub -V -l select=1:ncpus=${NCORES}:mem=8GB,walltime=1:00:00 -N MT_consensus -W depend=afterok:$CALL_VARS_ID | egrep -o "^[0-9]+")

# extract all reads mapping to Chr X or Y
parallel --dry-run "samtools faidx $GENOME.fasta X | bcftools consensus -H 1pIu -s {} ${BATCH.${ALIGNER}.${REF}.norm.vcf.gz > ${BATCH}.${ALIGNER}.${REF}.{}.chrX.fa" ::: $(zcat ${BATCH}.${ALIGNER}.${REF}.vcf.gz | head -n 2000 | grep '#CHROM' | cut -f 10- ) > extract_chrX.cmds
#parallel --dry-run "bcftools mpileup --threads \$NCPUS -f $GENOME.fasta -r {1} {2} | bcftools call  --threads \$NCPUS -c | vcfutils.pl vcf2fq >> {2.}.chrX.fq" ::: $( cat $REF.chrX )  ::: *.bam > extract_chrX.cmds
# create a pbs file
sed -E 's/mem=[0-9]+GB,walltime=[0-9]+:00:00/mem=4GB,walltime=0:30:00/'   ${RUN}.pbspro > extract_chrX.pbspro
# run jobs as an array
MT_CONSENS_ID=$( qsub -J1-$( cat extract_chrX.cmds | wc -l ) -N chrX_consensus  -vCMDS_FILE=extract_chrX.cmds -W depend=afterok:$CALL_VARS_ID extract_chrX.pbspro | egrep -o "^[0-9]+") 
echo "Finished extracting consensus sequence job array" | qsub -l select=1:ncpus=1:mem=4MB,walltime=00:01:00 -N cons_finish -m e -M i.bar@griffith.edu.au -W depend=afterok:$MT_CONSENS_ID[]
# transfer the sequences to CloudStor
conda activate base
rclone copy -L -P `pwd`/ChrX_consensus_seqs CloudStor:Shared/GRIFFITH-Ford-Crop-Genetics-Lab/Dingo/ChrX_consensus_seqs
# run jobs in a single node
# CHRX_ID=$( echo "cd $( pwd ) ; source ~/.bashrc; conda activate $CONDA_NAME; cat extract_chrX.cmds | parallel"  | qsub -l select=1:ncpus=${NCORES}:mem=32GB,walltime=3:00:00 -N extract_chrX | egrep -o "^[0-9]+") 

# find and remove empty files
find . -size 0 -exec rm {} + 

echo "########################### Job Execution History #############################"
echo "Completed the following run: '$RUN' on $(date)"
echo -e "Bowtie2 mapping details:\n\tArrayID = $ALIGN_ID"
qstat -xftw $ALIGN_ID[] | egrep "resources_used|Exit_status|array_index = " # | gwak -v ORS="; " '1'
echo -e "Post-trim QC details:\n\tJobID = $QC_JOB_ID"
qstat -xftw $QC_JOB_ID | egrep "resources_used|Exit_status"
echo -e "Qualimap details:\n\tJobID = $QUALIMAP_JOB_ID" # , $(grep 'ExitStatus' *.e$QUALIMAP_JOB_ID*)"
qstat -xftw $QUALIMAP_JOB_ID | grep "resources_used|Exit_status"
echo -e "MultiQC details:\n\tJobID = $MULTIQC_JOB_ID"
qstat -xftw $MULTIQC_JOB_ID | grep "resources_used|Exit_status"
echo -e "Mapping stats details:\n\tJobID = $STATS_JOB_ID"
qstat -xftw $STATS_JOB_ID | grep "resources_used|Exit_status"
echo -e "Final BAM merge details:\n\tJobID = $FINAL_MERGE_ID"
qstat -xftw $FINAL_MERGE_ID | grep "resources_used|Exit_status"
echo -e "Software used:\n\t bowtie2 v$(bowtie2 --version 2>&1 | gawk 'NR==1{print $NF}')\n\t $(samblaster --version 2>&1)\n\t $(bbduk.sh --version 2>&1 | grep "BBMap version")\n\t $(sambamba --version 2>&1 | head -n1)\n\t Picard $(picard ViewSam --version 2>&1)\n\t $(samtools --version 2>&1 | head -n1)\n\t $(fastqc --version 2>&1)\n\t $(qualimap --version 2>&1 | gawk 'NR==4')\n\t $(multiqc --version 2>&1)" 
# Check that all jobs finished successfuly
# find . -regextype posix-egrep -regex '\./.*\.e[0-9]{5}.*' | xargs grep "ExitStatus" #  *m.e$JOB_ID.*
END_TIME=$(date +%s)
TOTAL_SECS=$(( END_TIME - START_TIME ))
# LAPSED_TIME=$( date -ud "@${TOTAL_SECS}" + '$((%s/3600/24)) days %T' )
eval "echo $(date -ud "@$TOTAL_SECS" +'Total runtime: $((%s/3600/24)) days %T')"
echo "###############################################################################"


# Done!
```


#### Call Variants
After alignment, variants were called from the aligned read files with either `mpileup` (from `r fontFmt("samtools/bcftools/sambamba")`), GATK (UnifiedGenotyper or HaplotypeCaller), FreeBayes[@garrisonHaplotypebasedVariantDetection2012b]; or aDNA-specific tools, such as [`r fontFmt("pileupCaller")`](https://github.com/stschiff/sequenceTools), `r fontFmt("ARIADNA")` [@kawashARIADNAMachineLearning2018], `r fontFmt("AntCaller")` [@zhouAntCallerAccurateVariant2017] or `r fontFmt("snpAD")` [@pruferSnpADAncientDNA2018].  


#### Pipeline execution with Nextflow/EAGER 
The entire analysis workflow can be automated and run with `r fontFmt("EAGER")` (v2.2.2), which was developed specifically for processing of aDNA samples and includes all the tools and approaches mentioned above (and more) [@yatesReproduciblePortableEfficient2020]. `r fontFmt("EAGER")` is implemented through `r fontFmt("Nextflow")` (v20.10.0), a bioinformatics workflow management system [@ditommasoNextflowEnablesReproducible2017; @jacksonUsingRapidPrototyping2020].  

The pipeline parameters were set using the `r fontFmt("EAGER")` [Launcher web app](https://nf-co.re/launch?pipeline=eager&release=2.2.2) and saved to `params.json` file that was used as input for the run. Read files details were provided in the `WWR_samples.tsv` file.

_Since `nextflow` is not yet installed on Griffith HPC systems, this analysis was run at the QRIS Awoonga HPC_

```{bash nextflow}
conda install -n base nextflow nf-core
DATE=`date +%d_%m_%Y`
BATCH=WWR_NF1
ALIGNER=bt2-local # bwamem # bt2-local-N1  bwaaln 
GENOTYPER=gatkhc # pileupcaller
RUN="${BATCH}_process_${DATE}" # day of run was 02_02_2019
WORK_DIR=$HOME/aDNA/Human/WWR
# Create tmp folder
TMP_DIR=$HOME/30days/tmp && mkdir -p $TMP_DIR # on Awoonga
# setup environment variable to run singularity and nextflow
printf 'export SINGULARITY_CACHEDIR=/30days/ibar/.singularity\nexport SREGISTRY_DATABASE=$SINGULARITY_CACHEDIR\nexport NXF_SINGULARITY_CACHEDIR=$SINGULARITY_CACHEDIR\nexport NXF_CONDA_CACHEDIR=/30days/ibar/.conda\nexport NXF_OPTS="-Xms1g -Xmx4g"' >> ~/.bashcr
source ~/.bashrc
mkdir -p $SINGULARITY_CACHEDIR $NXF_CONDA_CACHEDIR

REF_DIR="/30days/ibar/data/Human/reference_genome"
REF="GRCh38"
[ ! -s "$REF_DIR/$REF.fasta" ] && pigz -cd $(find $REF_DIR -name "*$REF*.f*a.gz") > $REF_DIR/$REF.fasta
[ ! -s "$REF_DIR/$REF.gff" ] && pigz -cd $(find $REF_DIR -name "*$REF*.gff*.gz") > $REF_DIR/$REF.gff

#cat $REF_DIR/Dingo_MT_MH035676.1.fasta >> $REF_DIR/$REF.fasta

GENOME="$REF_DIR/$REF"
FQ_DIR=${WORK_DIR}/fastq_files
# copy input files from CloudStor
# ln -s ~/30days/data ~/aDNA # on Awoonga
# mkdir -p $FQ_DIR && cd $FQ_DIR
rclone copy -P CloudStor:aDNA/Human/WWR/fastq_files $FQ_DIR

# downsample files to test
mkdir sample5M
echo "source ~/.bashrc; conda activate aDNA; cd \$PBS_O_WORKDIR ;$(find $FQ_DIR -maxdepth 1 -name "UWW1_*_1.f*q.gz" | parallel --dry-run reformat.sh in='{= s/_1/_#/ =}' out={//}sample5M/'{= s/.+\\//; s/_1.+// =}'_#.sample5M.fq.gz samplereadstarget=5m )
" | qsub -N downsample -A qris-gu -l select=1:ncpus=8:mem=8GB,walltime=01:00:00

# create working folder for the run
mkdir -p ${WORK_DIR}/${RUN} # on Awoonga
cd !$

# create sample table

# wget https://raw.githubusercontent.com/nf-core/test-datasets/eager/reference/TSV_template.tsv
find $FQ_DIR -maxdepth 1 -name "UWW1*_1*.f*q.gz" | sort | gawk '{n=split($1,a,"/"); infile2=gensub("_1\\.", "_2.", "1", $1); match(a[n], /(.+)_1\..*f.*q.gz/, sample_ids); printf "%s\t%s\t1\t4\tPE\tHuman\tdouble\thalf\t%s\t%s\tNA\n", sample_ids[1], sample_ids[1], $1, infile2}' | cat <(curl https://raw.githubusercontent.com/nf-core/test-datasets/eager/reference/TSV_template.tsv) - > $BATCH.tsv

# Use the EAGER launcher to set run parameters and then edit the json file accordingly
sed -r "s/: \".+\.tsv\"/: \"$BATCH.tsv\"/" $WORK_DIR/WWR_aDNA_NF_default.json > $BATCH.$REF.$ALIGNER.$GENOTYPER.json
# or in subsequent runs when the bam files already exist
# sed -r "s/: \".+\.tsv\"/: \"$BATCH.tsv\"/" ../Dingo_aDNA_NF_reuseBAM.json > $BATCH.$REF.$ALIGNER.$GENOTYPER.json

# run EAGER with the parameters file and custom profile (qris.config)
nextflow run nf-core/eager -r 2.3.1 -params-file $BATCH.$REF.$ALIGNER.$GENOTYPER.json -c $HOME/.nextflow/qris.config -profile large_files

# for bowtie2 (to fix RG issue until a patch is provided)
nextflow run $HOME/.nextflow/assets/nf-core/eager_test/ -with-singularity $NXF_SINGULARITY_CACHEDIR/nfcore-eager-2.2.2.img -params-file $BATCH.$REF.$ALIGNER.$GENOTYPER.json -c $HOME/.nextflow/awoonga.config

# After each job finishes, create a folder to copy the relevant results to CloudStor

mkdir -p $WORK_DIR/CloudStor_copy/$RUN/results
find $WORK_DIR/$RUN -maxdepth 1 -type f -name "$BATCH*" | parallel ln -sf {} $WORK_DIR/CloudStor_copy/$RUN/
find $WORK_DIR/$RUN/results/ -maxdepth 1 -type d | egrep "results/multiqc|results/qualimap|results/genotyping" | parallel ln -sf {} $WORK_DIR/CloudStor_copy/$RUN/results/

# check number of SNPs in each sample
cd CloudStor_copy
find -L ./ -name *.vcf.gz  | parallel --dry-run "printf \"{}\t%s\t%s\t%s\t%s\n\" \$(zcat {} | grep -v -c '^#') \$(zcat {} | SnpSift filter \"( GEN[?].DP > 5 ) & ( GEN[?].GT != './.' )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -v -c '^#') \$(zcat {} | SnpSift filter \"( GEN[?].DP > 5 ) & ( GEN[?].GT != './.' ) & ( QUAL > 20 )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -v -c '^#') \$(zcat {} | SnpSift filter \"( GEN[?].DP > 5 ) & ( GEN[?].GT != './.' ) & ( QUAL > 30 )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -v -c '^#')" > vcf_stats.cmds

VCF_JOB=$(qsub -J1-$(cat vcf_stats.cmds | wc -l) -N vcf_stats -vCMDS_FILE=vcf_stats.cmds vcf_stats.pbspro | egrep -o "^[0-9]+")

cat <(printf "file\ttotal_snps\tDP_filtered_snps\tQUAL20_filtered_snps\tQUAL30_filtered_snps\n") <(cat vcf_stats.o$VCF_JOB*) > vcf_stats_$DATE.txt
rm vcf_stats.e$VCF_JOB* vcf_stats.o$VCF_JOB* 

# Finally copy the folder to CloudStor
rclone sync -P -L $WORK_DIR/CloudStor_copy/$RUN CloudStor:Shared/AncientDNA/Human/WWR/EAGER_results/$RUN

```

#### Variant filtration and analysis

##### Individual samples
Use the `vcf` file that was produced using the samples separately to determine the error rate and confirm that there are no contaminations and that they indeed originate from the same individual. To do that, we will filter the SNPs to include only ones that are present in all samples (`DP>5`), then calculate error rate. In addition, combine with external data (a subset of the 2,500 genomes SNPs) and calculate genetic distance and PCA.

Filtering criteria:
1. Genotype `DP>5`  
2. Genotype present in all samples  
3. Genotype is a SNP and not indel    
4. Genotype is not "weird" (`1|1:0,5:5:15:1|1:10507968_A_G:225,15,0:10507968`) - use `( ! exists GEN[?].PGT)`

Filtering was performed with a combination of SnpSift v4.3t [@ruden_using_2012] and gawk.

```{bash variant_filering}
# VariantFiltration 
# Number of variants from each sample
SAMPLES=$(zcat WWR.BT2.GRCh38.vcf.gz | head -n 2000 | grep '#CHROM' | cut -f 10-)
parallel --dry-run  "echo {} \$(zcat ${BATCH}.${ALIGNER}.${REF}.vcf.gz | SnpSift filter \"( GEN[{}].DP > 5 ) & ( GEN[{}].GT != './.' )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1))print}' | grep -c -v '^##')" ::: $SAMPLES

# this analysis on Griffith HPC
zcat ${BATCH}.${ALIGNER}.${REF}.vcf.gz | SnpSift filter "( GEN[?].DP > 5 ) & ( GEN[?].GT != './.' )" | gawk '{if ($0 ~ /^#/ || (length($4)==1 && length($5)==1))print}' | pigz -c > ${BATCH}.${ALIGNER}.${REF}.DP5.GT.snps.vcf.gz
# check how many variants left
zcat ${BATCH}.${ALIGNER}.${REF}.DP5.GT.snps.vcf.gz | grep -c -v "^##" # 1795
```


#### Combine samples
Once the samples were confirmed to be of the same individual and no contaminations, we can combine data and run the pipeline as a single sample. It is best **NOT** to combine the raw data, but define all relevant samples (in our case Bone and Powdered Bone) as the same `Sample_name` in the EAGER sample table (leave them as separate libraries) and let EAGER handle the merging of the data (which is done only after each sample was processed, QC-assessed, mapped and dedupped individually)

```{bash eager_combined}
conda install -n base nextflow nf-core
DATE=`date +%d_%m_%Y`
BATCH=WWR_NF2_combined
ALIGNER=bwamem # bt2-local-N1  bwaaln  bwamem
GENOTYPER=gatkhc # gatkug  freebayes
RUN="${BATCH}_process_${DATE}" # day of run was 02_02_2019
# WORK_DIR=/30days/ibar/data/Human/WWR 
WORK_DIR=/30days/sallywasef/WWR # This was run on Sally's Awoonga account

REF_DIR="/30days/ibar/data/Human/reference_genomes"
REF="GRCh38"
GENOME="$REF_DIR/$REF"
[ ! -s "$REF_DIR/$REF.fasta" ] && pigz -cd $(find $REF_DIR -iname "*$REF*.f*a.gz") > $GENOME.fasta
[ ! -s "$REF_DIR/$REF.gff" ] && pigz -cd $(find $REF_DIR -iname "*$REF*.gff*.gz") > $GENOME.gff
#cat $REF_DIR/Dingo_MT_MH035676.1.fasta >> $REF_DIR/$REF.fasta


FQ_DIR=/30days/ibar/data/Human/WWR/fastq_files
# create working folder for the run
mkdir -p ${WORK_DIR}/${RUN} # on Awoonga
cd !$

# create sample table
# wget https://raw.githubusercontent.com/nf-core/test-datasets/eager/reference/TSV_template.tsv
find $FQ_DIR -maxdepth 1 -name "UWW1_B*_1*.f*q.gz" | sort | gawk '{n=split($1,a,"/"); infile2=gensub("_1\\.fa", "_2.fa", "1", $1); match(a[n], /(.+)_1\..*f.*q.gz/, sample_ids); printf "Bone\t%s\t1\t4\tPE\tHuman\tdouble\thalf\t%s\t%s\tNA\n", sample_ids[1], $1, infile2}' | cat <(curl https://raw.githubusercontent.com/nf-core/test-datasets/eager/reference/TSV_template.tsv) - > $BATCH.tsv


# Use the EAGER launcher to set run parameters and then edit the json file accordingly
sed -r "s/: \".+\.tsv\"/: \"$BATCH.tsv\"/" $WORK_DIR/WWR_aDNA_NF_default.json > $BATCH.$REF.$ALIGNER.$GENOTYPER.json
# or in subsequent runs when the bam files already exist
# sed -r "s/: \".+\.tsv\"/: \"$BATCH.tsv\"/" ../Dingo_aDNA_NF_reuseBAM.json > $BATCH.$REF.$ALIGNER.$GENOTYPER.json

# run EAGER with the parameters file and custom profile (qris.config)
nextflow run /home/ibar/.nextflow/assets/nf-core/eager_test/ -params-file $BATCH.$REF.$ALIGNER.$GENOTYPER.json -c /home/ibar/.nextflow/qris.config 
# or development version
nextflow-test run /home/ibar/etc/tools/nf-core/eager -with-singularity $NXF_SINGULARITY_CACHEDIR/nfcore-eager-2.2.2.img -params-file $BATCH.$REF.$ALIGNER.$GENOTYPER.json -c $HOME/.nextflow/qris.config

# After each job finishes, create a folder to copy the relevant results to CloudStor

mkdir -p $WORK_DIR/CloudStor_copy/$RUN/results
find $WORK_DIR/$RUN -maxdepth 1 -type f -name "$BATCH*" | parallel ln -sf {} $WORK_DIR/CloudStor_copy/$RUN/
find $WORK_DIR/$RUN/results/ -maxdepth 1 -type d | egrep "results/multiqc|results/qualimap|results/genotyping" | parallel ln -sf {} $WORK_DIR/CloudStor_copy/$RUN/results/

# check number of SNPs in each sample
cd CloudStor_copy
find -L ./ -name *.vcf.gz  | parallel --dry-run "printf \"{}\t%s\t%s\t%s\t%s\n\" \$(zcat {} | grep -v -c '^#') \$(zcat {} |  SnpSift varType - |  SnpSift filter \"( GEN[?].DP > 5 ) & ( GEN[?].GT != './.' )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -v -c '^#') \$(zcat {} | SnpSift filter \"( GEN[?].DP > 5 ) & ( GEN[?].GT != './.' ) & ( QUAL > 20 )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -v -c '^#') \$(zcat {} | SnpSift filter \"( GEN[?].DP > 5 ) & ( GEN[?].GT != './.' ) & ( QUAL > 30 )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -v -c '^#')" > vcf_stats.cmds

VCF_JOB=$(qsub -J1-$(cat vcf_stats.cmds | wc -l) -N vcf_stats -vCMDS_FILE=vcf_stats.cmds vcf_stats.pbspro | egrep -o "^[0-9]+")

cat <(printf "file\ttotal_snps\tDP_filtered_snps\tQUAL20_filtered_snps\tQUAL30_filtered_snps\n") <(cat vcf_stats.o$VCF_JOB*) > vcf_stats_$DATE.txt
rm vcf_stats.e$VCF_JOB* vcf_stats.o$VCF_JOB* 

# Finally copy the folder to CloudStor
rclone sync -P -L $WORK_DIR/CloudStor_copy/$RUN CloudStor:Shared/AncientDNA/Human/WWR/EAGER_results/$RUN
```

#### Match SNPs with 1000 genomes data 

SNP data of 3,205 individuals from the 1000 Genome project (30x coverage GRCh38 dataset) was downloaded from the [data portal](https://www.internationalgenome.org/data-portal/data-collection/30x-grch38). The files were then processed to remove duplicate variants and normalise multi-allelic variants, convert to `bcf` format and then to PLINK to filter for informative markers (see this [tutorial on BioStars](https://www.biostars.org/p/335605/)).


```{bash 1000g_download}
# download reference genome
REF=GRCh38_full_analysis_set_plus_decoy_hla.fa
mkdir -p ~/data/Q1369/Genomes/Human
cd ~/data/Q1369/Genomes/Human
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/$REF
CONDA_NAME=snp_tools
conda activate $CONDA_NAME
samtools faidx $REF
GENOME=$HOME/data/Q1369/Genomes/Human/$REF
# download vcf files to Awoonga (Sally's account)
mkdir -p ~/data/Q1369/Genomes/SNP_db/igsr_30x_GRCh38 
cd !$
# mkdir vcf_test_reports
# copy the file containing the links or download from the data portal at https://www.internationalgenome.org/data-portal/data-collection/30x-grch38
# download the files
tail -n +2 igsr_30x_GRCh38.tsv | cut -f 1 | grep -v ".tbi$" | parallel --dry-run "conda activate $CONDA_NAME; wget -c {} && wget -c {}.tbi && gzip -t {/} && mv {/}* $DATA_DIR/" > get_vcfs.cmds
GET_VCFS=$(qsub -J1-$(cat get_vcfs.cmds | wc -l) -l select=1:ncpus=1:mem=8GB,walltime=5:00:00 -N get_vcf_files -vCMDS_FILE=get_vcfs.cmds ~/data/Q1369/Genomes/SNP_db/get_files.pbspro | egrep -o "^[0-9]+")
# verify that all jobs completed successfully
grep "ExitStatus" *.e$GET_VCFS*

```

Alternatively (due to I/O errors in `/data/Q1369` folder), download, verify and process each file in the scratch folder (`/30days`).

```{bash process_1000g_vcf}
WORK_DIR=$HOME/30days/WWR/1000g_data/igsr_30x_GRCh38
DATA_DIR=$HOME/data/Q1369/Genomes/SNP_db/igsr_x30_GRCh38 # (copy from Qdata currently not working)
CONDA_NAME=snp_tools
# make working dir and prepare input files
OUT_PRUNE=pruned_plink_files
OUT_NORM=norm_bcfs
mkdir -p $DATA_DIR/$OUT_NORM
mkdir -p $WORK_DIR/$OUT_NORM/$OUT_PRUNE
cd $WORK_DIR
REF=GRCh38_full_analysis_set_plus_decoy_hla.fa
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/$REF
conda activate $CONDA_NAME
samtools faidx $REF

RUN=vcf_to_plink
# mkdir test_vcf_reports
cp /home/sallywasef/data/Q1369/Genomes/SNP_db/igsr_x30_GRCh38/*.tsv ./
# with such a long processing - wrap this in a simple bash script and call it every time (this will allow proper error handling, etc.)
tail -n +2 igsr_30x_GRCh38.tsv | cut -f 1 | egrep -v ".tbi|chrY|chrX|others" | \
parallel -k --dry-run "basename={= s:.*/::; s/.vcf.gz// =}; recall_medici $DATA_DIR/{/}* && cp $DATA_DIR/{/}* ./ && wget -c {} && wget -c {}.tbi &&  gzip -t {/}  && bcftools norm -m-any --check-ref w -f $REF -Ou {/} | bcftools annotate -x ID -I +'%CHROM:%POS:%REF:%ALT' -Ou | bcftools norm -Ob --threads \$NCPUS -m -snps --rm-dup both > $OUT_NORM/\$basename.norm.bcf.bz2 && bcftools index $OUT_NORM/\$basename.norm.bcf.bz2 && bgzip -t $OUT_NORM/\$basename.norm.bcf.bz2 && rm {/}* && plink --bcf $OUT_NORM/\$basename.norm.bcf.bz2 --keep-allele-order --snps-only --vcf-idspace-to _ --double-id --allow-extra-chr 0 --split-x b38 no-fail --make-bed --out $OUT_NORM/\$basename.genotypes && plink --bfile $OUT_NORM/\$basename.genotypes --maf 0.10 --indep 50 5 1.5 --out $OUT_NORM/$OUT_PRUNE/\$basename.genotypes && plink --bfile $OUT_NORM/\$basename.genotypes --extract $OUT_NORM/$OUT_PRUNE/\$basename.genotypes.prune.in --make-bed --out $OUT_NORM/$OUT_PRUNE/\$basename.genotypes && mv $OUT_NORM/\$basename.norm.bcf.bz2* $DATA_DIR/$OUT_NORM/ ; rm $OUT_NORM/\$basename.norm.bcf.bz2*" > $WORK_DIR/$RUN.cmds


echo '#!/bin/bash 
#PBS -V
#PBS -W umask=002

cd $WORKDIR
source ~/.bashrc'"
conda activate $CONDA_NAME
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > array.pbspro

cd pbs_logs
# run for all files
STEP_SIZE=4
PROCESS_VCFS=$(qsub -J1-$(cat $WORK_DIR/$RUN.cmds | wc -l):$STEP_SIZE -l select=1:ncpus=2:mem=16GB,walltime=20:00:00 -N ${RUN:0:11} -A qris-gu -vCMDS_FILE=$WORK_DIR/$RUN.cmds array.pbspro -vWORKDIR=$WORK_DIR | egrep -o "^[0-9]+")
PROCESS_VCFS2=$(qsub -J2-$(cat $WORK_DIR/$RUN.cmds | wc -l):$STEP_SIZE -l select=1:ncpus=2:mem=16GB,walltime=20:00:00 -N ${RUN:0:11} -W depend=afterok:$PROCESS_VCFS[] -A qris-gu -vCMDS_FILE=$WORK_DIR/$RUN.cmds array.pbspro | egrep -o "^[0-9]+")
PROCESS_VCFS3=$(qsub -J3-$(cat $WORK_DIR/$RUN.cmds | wc -l):$STEP_SIZE -l select=1:ncpus=2:mem=16GB,walltime=20:00:00 -N ${RUN:0:11} -W depend=afterok:$PROCESS_VCFS2[] -A qris-gu -vCMDS_FILE=$WORK_DIR/$RUN.cmds array.pbspro | egrep -o "^[0-9]+")
PROCESS_VCFS4=$(qsub -J4-$(cat $WORK_DIR/$RUN.cmds | wc -l):$STEP_SIZE -l select=1:ncpus=2:mem=16GB,walltime=20:00:00 -N ${RUN:0:11} -W depend=afterok:$PROCESS_VCFS3[] -A qris-gu -vCMDS_FILE=$WORK_DIR/$RUN.cmds array.pbspro | egrep -o "^[0-9]+")
# check for successful completion
grep "ExitStatus" ${RUN:0:11}.e*
# remove empty log files
find . -type f -size 0 -name "${RUN:0:11}.*" -exec rm {} +
```

The resulting WWR `vcf` file from EAGER was processed the same way to retain highly informative SNPs for comparison with the 1000 Genomes data. It may be better to merge the 2 datasets first based on common SNPs before pruning the files to increase the chance of finding overlapping markers. Need to try both approaches and compare.

```{bash process_WWR}
WORK_DIR=$HOME/30days/WWR/WWR_NF2_genotyping_process_27_01_2021/results/genotyping
CONDA_NAME=snp_tools
# make working dir and prepare input files
cd $WORK_DIR
REF=/30days/ibar/data/Human/reference_genomes/GRCh38.fasta
conda activate $CONDA_NAME
samtools faidx $REF
OUT_PRUNE=pruned_plink_files
OUT_NORM=norm_bcfs
mkdir -p $OUT_NORM/$OUT_PRUNE
RUN=vcf_to_plink
BASENAME=UWW1_B.haplotypecaller.norm
# mkdir test_vcf_reports
# convert gvcf to vcf (should start from original HaplotypeCaller gvcf)
#echo "source ~/.bashrc; conda activate $CONDA_NAME; cd \$PBS_O_WORKDIR; bcftools view -Oz -e 'ALT=\"<NON_REF>\"' $BASENAME.bcf.bz2 > $BASENAME.called_vars.vcf.gz && bcftools view -Oz -i 'ALT=\"<NON_REF>\"' $BASENAME.bcf.bz2 > $BASENAME.g.vcf.gz && gatk IndexFeatureFile -I $BASENAME.g.vcf.gz && gatk --java-options "-Xmx4g" GenotypeGVCFs -R $REF -V $BASENAME.g.vcf.gz -O $BASENAME.vcf.gz" | qsub -l select=1:ncpus=4:mem=16GB,walltime=5:00:00 -N gvcf_to_vcf -A qris-gu
cd $OUT_NORM
# extract only valid variants from the "gvcf" file
bcftools view -Ov -e 'ALT=\"<NON_REF>\"' $BASENAME.bcf.bz2 | gawk '{sub(/^([0-9]+)/, "chr&", $0); print $0}' | egrep "^#|^chr" | bcftools view -Oz - > tmp.vcf.gz 
bcftools index tmp.vcf.gz
bcftools annotate -x ID -I +'%CHROM:%POS:%REF:%ALT' -Ou tmp.vcf.gz | bcftools view -Ob > $BASENAME.called_vars.fix_chr.bcf.bz2

# extract variants to use in filtering
bcftools view UWW1_B.haplotypecaller.norm.called_vars.fix_chr.bcf.bz2 | grep -v "^#" | cut -f 3 > UWW1_B.snplist

# or this:
bcftools view UWW1_B.haplotypecaller.norm.called_vars.fix_chr.bcf.bz2 | grep -v "^#" | gawk '{print $1, $2, $2}' > UWW1_B.snp_range.list

# sort files and combine
#echo "source ~/.bashrc; conda activate $CONDA_NAME; cd \$PBS_O_WORKDIR; bcftools sort -T \$TMPDIR -Ob $BASENAME.vcf.gz > $BASENAME.sorted.bcf.bz2 &&  cat $BASENAME.vcf.tmp | bcftools sort -T \$TMPDIR -Ob - > $BASENAME.tmp.sorted.bcf.bz2 && bcftools concat -a -d both -Ou $BASENAME.sorted.bcf.bz2 $BASENAME.tmp.sorted.bcf.bz2 | bcftools norm -m-any --check-ref w -f $REF -Ou | bcftools annotate -x ID -I +'%CHROM:%POS:%REF:%ALT' -Ou | bcftools norm -Ob --threads \$NCPUS $BASENAME.processed.bcf.bz2" | qsub -l select=1:ncpus=4:mem=16GB,walltime=10:00:00 -N process_vcf -A qris-gu
# "fix" chromosome information
#bcftools view -Ov UWW1_B.haplotypecaller.norm.bcf.bz2 |  gawk '{sub(/^([0-9]+)/, "chr&", $0); print $0}' | egrep "^#|^chr" > UWW1_B.haplotypecaller.norm.fix_chr.vcf
# make plink file
cd $WORK_DIR
echo "source ~/.bashrc; conda activate $CONDA_NAME; cd \$PBS_O_WORKDIR; plink --bcf $OUT_NORM/$BASENAME.called_vars.fix_chr.bcf.bz2 --keep-allele-order --vcf-idspace-to _ --double-id --allow-extra-chr 0 --split-x b38 no-fail --make-bed --out $OUT_NORM/$BASENAME.called_vars.fix_chr.genotypes"| qsub -l select=1:ncpus=4:mem=16GB,walltime=5:00:00 -N bcf_to_plink -A qris-gu

```

After the files have been downloaded, normalised and processed into `plink` files, they can be concatenated (while keeping SNPs only) to find overlapping variants with the WWR sample (produced by EAGER). This is done by converting the files to `bcf` format with `plink2`, then concatenating with `bcftools`, identifying common SNPs and finally converting back to `plink` format.

```{bash merge_plinks}
# find common vars
WORK_DIR=$HOME/30days/WWR/1000g_data/igsr_30x_GRCh38
cd $WORK_DIR/pbs_logs
RUN=find_common_vars
find $WORK_DIR/$OUT_NORM -maxdepth 1 -name "20201028_CCDG_14151_B01_GRM_WGS_2020-08-05*.bed" | parallel --dry-run "plink2 --bfile {.} --extract $HOME/90days/WWR/WWR_NF2_genotyping_process_27_01_2021/results/genotyping/norm_bcfs/UWW1_B.snp_range.list --make-bed --out {.}.common_vars" > $WORK_DIR/$RUN.cmds

cd pbs_logs

# run all commands
FIND_COMMON_VARS=$(qsub -J1-$(cat $WORK_DIR/$RUN.cmds | wc -l) -l select=1:ncpus=2:mem=16GB,walltime=5:00:00 -N ${RUN:0:11} -A qris-gu -vCMDS_FILE=$WORK_DIR/$RUN.cmds -v WORKDIR=$WORK_DIR array.pbspro | egrep -o "^[0-9]+")
# check for successful completion
grep "ExitStatus" ${RUN:0:11}.e*
# remove empty log files
find . -type f -size 0 -name "${RUN:0:11}.*" -exec rm {} +

cd $WORK_DIR
# merge files
find $WORK_DIR/$OUT_NORM -maxdepth 1 -name "*common_vars.bed" | sed  's/.bed//' > norm_plink_files_to_merge.list
find $HOME/90days/WWR/WWR_NF2_genotyping_process_27_01_2021/results/genotyping/norm_bcfs -name "UWW1_B.haplotypecaller.norm.called_vars.fix_chr.genotypes.bed" | sed  's/.bed//' >> norm_plink_files_to_merge.list

cd pbs_logs
# merge files and prune file
echo "outfile=igsr_30x_GRCh38_allChr.norm.common_vars; cd $WORK_DIR; source ~/.bashrc; conda activate snp_tools ; plink --merge-list norm_plink_files_to_merge.list --make-bed --snps-only --out $OUT_NORM/\$outfile && plink --bfile $OUT_NORM/\$outfile --maf 0.10 --indep 50 5 1.5 --out $OUT_NORM/$OUT_PRUNE/\$outfile && plink --bfile $OUT_NORM/\$outfile --extract $OUT_NORM/$OUT_PRUNE/\$outfile.prune.in --make-bed --out $OUT_NORM/$OUT_PRUNE/\$outfile" | qsub -l select=1:ncpus=2:mem=16GB,walltime=5:00:00 -N merge_prune -A qris-gu

# make PCA out of pruned file
plink --bfile $WORK_DIR/$OUT_NORM/$OUT_PRUNE/igsr_30x_GRCh38_allChr.norm.common_vars --pca
```

#### PCA analysis
Import PCA data (produced by `PLINK`) and visualise by population.  
Select only individuals from EUR populations (see `R_1000g.R` file), to try and identify the sample's origin by its clustering.

```{r superpop_PCA}

```

#### Variant Annotation
Annotate the variants with SnpEff/SnpSift
```{bash var_annotation}
java -jar SnpSift.jar annotate -dbsnp file.vcf \
    | java -Xmx8g -jar snpEff.jar eff -v GRCh37.75 - \
    | java -jar SnpSift.jar filter "! exists ID" \
    > file.ann.not_in_dbSnp.vcf
```
